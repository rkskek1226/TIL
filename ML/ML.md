# ML

* 퍼셉트론

  가중치는 입력 신호가 결과에 주는 영향력을 조절하는 매개변수

  편향은 얼마나 쉽게 활성화하는지를 조절하는 매개변수

  단층 퍼셉트론은 비선형 영역을 분리할 수 없음(다층 퍼셉트론은 가능)





* 신경망

  가중치 매개변수를 자동으로 학습함

  입력층 -> n개의 은닉층 -> 출력층

  활성화 함수(activation function) : 입력 신호의 총합을 출력 신호로 변환하는 함수(비션형 함수)

  * 시그모이드 함수(sigmoid funcion) : 1/(1+exp(-x)) -> 1/(1+np.exp(-x)), 회귀에 주로 사용
  
  * ReLU 함수(Rectified Linear Unit function) : h(x) = x(x>0), 0(x<=0)
  
  * 소프트맥스 함수(softmax function) : 입력 신호의 지수 함수/모든 입력 신호의 지수 함수의 합, 출력이 0~1.0 사이의 실수로 출력의 총합이 1 -> 문제를 확률적(통계적)으로 대응 가능, 분류에 주로 사용
  
    
  
* 신경망 학습

  학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것

  오버피팅(overfitting) : 특정 데이터셋에만 최적화되어 다른 데이터셋에서는 효과적이지 않은 상태

  손실 함수(loss function), 비용 함수(cost function) : 가중치 매개변수의 최적값을 찾도록 만들어주는 함수

  * 오차 제곱합(SSE : Sum of squares for error) : 0.5*np.sum((y-t)**2)
  * 교차 엔트로피 오차(CCE : cross entropy error)